{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"John like to watch movies\",\n",
    "         \"Mary likes to play football\",\n",
    "         \"John likes to watch football games but does not like to play football\",\n",
    "         \"Both John and Mary like to play video games\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John like to watch movies', 'Mary likes to play football', 'John likes to watch football games but does not like to play football', 'Both John and Mary like to play video games']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shouldn', 'y', 'more', \"wasn't\", 'into', 'each', 'yourselves', 'itself', 'her', 'needn', 'below', 'were', 'aren', 'before', 'any', 'it', 'for', 'same', 'off', 'our', 'them', 'will', 'o', 'so', 'mustn', 'just', \"should've\", 'where', 'of', 'shan', 'your', 'been', 'him', 'they', 'or', 'too', 'she', \"don't\", 'haven', 'did', 'from', 'he', 'being', 'an', 'further', 'once', 'after', 'again', 'and', 'no', 'own', 'hadn', 'wasn', \"you've\", 'a', 'until', 'd', 'll', 've', 'yourself', 'why', 'does', 'herself', 'very', \"won't\", 'down', 'other', 'few', \"shan't\", 'most', \"doesn't\", 'themselves', 'during', 'theirs', 'am', 'i', 'nor', 'on', \"that'll\", 'with', 'through', 'under', \"hasn't\", 'doesn', 'some', 'himself', 'those', 'do', 'has', \"weren't\", \"couldn't\", 'that', 'their', 't', 'my', 'because', \"isn't\", 'up', 'hers', 'weren', \"mustn't\", 'be', \"aren't\", 'you', 'having', 'in', 'out', 'is', \"haven't\", 'there', \"she's\", 's', 'than', \"you're\", 'myself', 'was', 'how', 'to', 'between', 'can', 'had', 'these', 'such', 'here', \"it's\", 'about', 'are', 'against', 'ain', 'what', 'but', 'if', 'at', 'we', 'all', 'then', 'over', 'didn', \"didn't\", 'have', 'as', \"needn't\", 'should', 're', 'only', 'won', 'wouldn', \"wouldn't\", 'couldn', 'isn', 'ma', \"hadn't\", \"you'd\", 'now', 'yours', 'mightn', 'this', 'the', \"mightn't\", 'don', \"shouldn't\", 'by', 'both', 'ourselves', 'above', 'hasn', 'who', 'its', 'while', 'me', 'ours', 'when', 'whom', 'doing', 'not', 'm', \"you'll\", 'which', 'his'}\n"
     ]
    }
   ],
   "source": [
    "# represent all the stop words\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "print(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to remove all the stop words\n",
    "def remove_stopwords(sentence):\n",
    "    # tokenize the sentence\n",
    "    word_tokens = nltk.word_tokenize(sentence)\n",
    "    print(word_tokens)\n",
    "    \n",
    "    # remove all the stop words from the text\n",
    "    cleaned_text = [w.lower() for w in word_tokens if(w not in english_stopwords)]\n",
    "    return(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ram', 'and', 'Shyam', 'likes', 'to', 'play']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ram', 'shyam', 'likes', 'play']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"Ram and Shyam likes to play\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a vocabulary and remove all the duplicate words\n",
    "def tokenize_sentences(sentences):\n",
    "    \n",
    "    # initialize the empty word\n",
    "    word = []\n",
    "    \n",
    "    # loop through each and every sentence\n",
    "    for x in sentences:\n",
    "        # call the function to remove all the stop words\n",
    "        words_new = remove_stopwords(x)\n",
    "        word.extend(words_new)\n",
    "        \n",
    "    # keep all the tokenized words in the list and sort it\n",
    "    sorted_words = list(sorted(word))\n",
    "    return(sorted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'like', 'to', 'watch', 'movies']\n",
      "['Mary', 'likes', 'to', 'play', 'football']\n",
      "['John', 'likes', 'to', 'watch', 'football', 'games', 'but', 'does', 'not', 'like', 'to', 'play', 'football']\n",
      "['Both', 'John', 'and', 'Mary', 'like', 'to', 'play', 'video', 'games']\n",
      "['both', 'football', 'football', 'football', 'games', 'games', 'john', 'john', 'john', 'like', 'like', 'like', 'likes', 'likes', 'mary', 'mary', 'movies', 'play', 'play', 'play', 'video', 'watch', 'watch']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = tokenize_sentences(corpus)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a bag of array of the frequency count\n",
    "def bag_of_count_vectors(sentence, words):\n",
    "    # remove all the stop words\n",
    "    sent = remove_stopwords(sentence)\n",
    "    \n",
    "    # create an array for the bag of words vector\n",
    "    bag = np.zeros(len(words))\n",
    "\n",
    "    # To check if the word is present in the dictionaty\n",
    "    for pol in sent:\n",
    "        for i, word in enumerate(words):\n",
    "            if(word == pol):\n",
    "                bag[i] += 1\n",
    "                \n",
    "    # to create an array for the bag of words\n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'like', 'to', 'watch', 'movies']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_count_vectors(\"John like to watch movies\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "information = [\"David like to watch TV\",\n",
    "         \"Shyam likes to play football\",\n",
    "         \"Max likes to watch baseball games but does not like to play football\",\n",
    "         \"Both Reena and Maria like to play video games\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['David like to watch TV', 'Shyam likes to play football', 'Max likes to watch baseball games but does not like to play football', 'Both Reena and Maria like to play video games']\n"
     ]
    }
   ],
   "source": [
    "# print the corpus\n",
    "print(information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shouldn', 'y', 'more', \"wasn't\", 'into', 'each', 'yourselves', 'itself', 'her', 'needn', 'below', 'were', 'aren', 'before', 'any', 'it', 'for', 'same', 'off', 'our', 'them', 'will', 'o', 'so', 'mustn', 'just', \"should've\", 'where', 'of', 'shan', 'your', 'been', 'him', 'they', 'or', 'too', 'she', \"don't\", 'haven', 'did', 'from', 'he', 'being', 'an', 'further', 'once', 'after', 'again', 'and', 'no', 'own', 'hadn', 'wasn', \"you've\", 'a', 'until', 'd', 'll', 've', 'yourself', 'why', 'does', 'herself', 'very', \"won't\", 'down', 'other', 'few', \"shan't\", 'most', \"doesn't\", 'themselves', 'during', 'theirs', 'am', 'i', 'nor', 'on', \"that'll\", 'with', 'through', 'under', \"hasn't\", 'doesn', 'some', 'himself', 'those', 'do', 'has', \"weren't\", \"couldn't\", 'that', 'their', 't', 'my', 'because', \"isn't\", 'up', 'hers', 'weren', \"mustn't\", 'be', \"aren't\", 'you', 'having', 'in', 'out', 'is', \"haven't\", 'there', \"she's\", 's', 'than', \"you're\", 'myself', 'was', 'how', 'to', 'between', 'can', 'had', 'these', 'such', 'here', \"it's\", 'about', 'are', 'against', 'ain', 'what', 'but', 'if', 'at', 'we', 'all', 'then', 'over', 'didn', \"didn't\", 'have', 'as', \"needn't\", 'should', 're', 'only', 'won', 'wouldn', \"wouldn't\", 'couldn', 'isn', 'ma', \"hadn't\", \"you'd\", 'now', 'yours', 'mightn', 'this', 'the', \"mightn't\", 'don', \"shouldn't\", 'by', 'both', 'ourselves', 'above', 'hasn', 'who', 'its', 'while', 'me', 'ours', 'when', 'whom', 'doing', 'not', 'm', \"you'll\", 'which', 'his'}\n"
     ]
    }
   ],
   "source": [
    "# define all the english stopwords\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "print(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def remove_stopwords(sentence):\n",
    "    # perform the word tokenization on the sentences\n",
    "    word_tok = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    words = []\n",
    "    # remove the stopwords from the sentence\n",
    "    for i in word_tok:\n",
    "        if(i not in english_stopwords):\n",
    "            words.append(i)\n",
    "    return(words)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['David', 'like', 'watch', 'TV']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the function to remove all the stop words\n",
    "remove_stopwords('David like to watch TV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a vocabulary and remove all the stop words from the sentences\n",
    "def build_vocabulary(sentence):\n",
    "    # call the remove stop words function to remove all the stop words from the sentences\n",
    "    words = []\n",
    "    for x in sentence:\n",
    "        words_data = remove_stopwords(x)\n",
    "        # append all the new data into the words list\n",
    "        words.extend(words_data)\n",
    "        \n",
    "        \n",
    "    # sort all the words in the list\n",
    "    sorted_words = list(sorted(words))\n",
    "    return(sorted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Both', 'David', 'Maria', 'Max', 'Reena', 'Shyam', 'TV', 'baseball', 'football', 'football', 'games', 'games', 'like', 'like', 'like', 'likes', 'likes', 'play', 'play', 'play', 'video', 'watch', 'watch']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = build_vocabulary(information)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To perform the bag of count function\n",
    "def bag_of_count_frequency(sentence, worrds):\n",
    "    # To call the remove stopwords function to remove all the stop words from the text\n",
    "    ans = remove_stopwords(sentence)\n",
    "    \n",
    "    \n",
    "    # To define the array of zeros fro all the words present in the information corpus\n",
    "    bag = np.zeros(len(worrds))\n",
    "    \n",
    "    \n",
    "    # To check if the word is present in the dictionary\n",
    "    for a in ans:\n",
    "        for i, sorted_words in enumerate(worrds):\n",
    "            if(sorted_words == a):\n",
    "                bag[i] += 1\n",
    "                \n",
    "    # To retuen the bag of count vectors\n",
    "    return np.array(bag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To call bag_of_count function\n",
    "bag_of_count_frequency('David like to watch TV', vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
